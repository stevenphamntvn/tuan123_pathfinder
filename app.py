# file: app.py
# ƒê√£ x√≥a c√°c d√≤ng __import__('pysqlite3') kh√¥ng c·∫ßn thi·∫øt

import streamlit as st
import chromadb
import google.generativeai as genai

# --- PH·∫¶N C·∫§U H√åNH ---
# API Key c·ªßa b·∫°n t·ª´ Google Cloud
GOOGLE_API_KEY = 'AIzaSyBOAgpJI1voNNxeOC6sS7y01EJRXWSK0YU' # !!! THAY API KEY C·ª¶A B·∫†N V√ÄO ƒê√ÇY !!!
# ƒê∆∞·ªùng d·∫´n t·ªõi c∆° s·ªü d·ªØ li·ªáu vector
DB_PATH = 'yhct_chroma_db'
# T√™n c·ªßa b·ªô s∆∞u t·∫≠p trong database
COLLECTION_NAME = 'yhct_collection'
# T√™n m√¥ h√¨nh b·∫°n mu·ªën s·ª≠ d·ª•ng
MODEL_NAME = 'gemini-1.5-pro-latest'

# --- B·∫¢NG GI√Å (C·∫≠p nh·∫≠t th√°ng 7/2025 - Vui l√≤ng ki·ªÉm tra l·∫°i gi√° tr√™n trang c·ªßa Google) ---
# Gi√° cho m·ªói 1 tri·ªáu token
PRICE_PER_MILLION_INPUT_TOKENS = 3.50  # $3.50
PRICE_PER_MILLION_OUTPUT_TOKENS = 10.50 # $10.50

# --- KH·ªûI T·∫†O AI V√Ä DATABASE ---
@st.cache_resource
def load_models_and_db():
    """T·∫£i model AI v√† k·∫øt n·ªëi t·ªõi database ch·ªâ m·ªôt l·∫ßn duy nh·∫•t."""
    try:
        genai.configure(api_key=GOOGLE_API_KEY)
        llm_model = genai.GenerativeModel(MODEL_NAME)
        
        client = chromadb.PersistentClient(path=DB_PATH)
        collection = client.get_collection(name=COLLECTION_NAME)
        
        return llm_model, collection
    except Exception as e:
        st.error(f"L·ªói kh·ªüi t·∫°o: {e}")
        return None, None

# --- H√ÄM LOGIC X·ª¨ L√ù C√ÇU H·ªéI ---
def get_ai_response(question, model, collection):
    """L·∫•y c√¢u tr·∫£ l·ªùi t·ª´ AI v√† th√¥ng tin s·ª≠ d·ª•ng API."""
    # 1. T√¨m ki·∫øm trong DB
    results = collection.query(
        query_texts=[question],
        n_results=3
    )
    
    retrieved_docs = results['documents'][0]
    retrieved_sources = [meta['source'] for meta in results['metadatas'][0]]

    # 2. T·∫°o prompt
    context = "\n\n---\n\n".join(retrieved_docs)
    prompt = f"""D·ª±a v√†o c√°c th√¥ng tin tham kh·∫£o ƒë∆∞·ª£c cung c·∫•p d∆∞·ªõi ƒë√¢y, h√£y tr·∫£ l·ªùi c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng.
    
    Th√¥ng tin tham kh·∫£o:
    {context}
    
    C√¢u h·ªèi: {question}
    """
    
    # 3. G·ªçi Gemini
    response = model.generate_content(prompt)
    
    # 4. Tr√≠ch xu·∫•t th√¥ng tin s·ª≠ d·ª•ng t·ª´ ph·∫£n h·ªìi c·ªßa API
    usage_info = None
    try:
        usage = response.usage_metadata
        prompt_tokens = usage.prompt_token_count
        response_tokens = usage.candidates_token_count
        total_tokens = usage.total_token_count
        
        # T√≠nh to√°n chi ph√≠
        input_cost = (prompt_tokens / 1_000_000) * PRICE_PER_MILLION_INPUT_TOKENS
        output_cost = (response_tokens / 1_000_000) * PRICE_PER_MILLION_OUTPUT_TOKENS
        total_cost = input_cost + output_cost
        
        usage_info = {
            "model": MODEL_NAME,
            "prompt_tokens": prompt_tokens,
            "response_tokens": response_tokens,
            "total_tokens": total_tokens,
            "cost_usd": total_cost
        }
    except Exception as e:
        print(f"Kh√¥ng th·ªÉ l·∫•y th√¥ng tin s·ª≠ d·ª•ng: {e}")

    return response.text, set(retrieved_sources), usage_info

# --- GIAO DI·ªÜN NG∆Ø·ªúI D√ôNG STREAMLIT ---
st.set_page_config(page_title="Tr·ª£ l√Ω Y h·ªçc C·ªï truy·ªÅn", page_icon="üåø")
st.title("üåø Tr·ª£ l√Ω Y h·ªçc C·ªï truy·ªÅn")
st.write("ƒê·∫∑t c√¢u h·ªèi ƒë·ªÉ tra c·ª©u ki·∫øn th·ª©c t·ª´ kho d·ªØ li·ªáu y h·ªçc c·ªï truy·ªÅn.")

llm_model, collection = load_models_and_db()

if llm_model and collection:
    if "messages" not in st.session_state:
        st.session_state.messages = []

    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"], unsafe_allow_html=True)

    if prompt := st.chat_input("V√≠ d·ª•: B·ªánh Th√°i D∆∞∆°ng l√† g√¨?"):
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        with st.chat_message("assistant"):
            with st.spinner("AI ƒëang ph√¢n t√≠ch v√† t·ªïng h·ª£p..."):
                response_text, sources, usage_info = get_ai_response(prompt, llm_model, collection)
                
                # T·∫°o n·ªôi dung hi·ªÉn th·ªã ch√≠nh
                source_markdown = "\n\n---\n**Ngu·ªìn tham kh·∫£o:**\n" + "\n".join([f"- `{s}`" for s in sources])
                full_response = response_text + source_markdown
                st.markdown(full_response)
                
                # Hi·ªÉn th·ªã th√¥ng tin s·ª≠ d·ª•ng API trong m·ªôt expander
                if usage_info:
                    with st.expander("Xem chi ti·∫øt s·ª≠ d·ª•ng API"):
                        col1, col2, col3, col4 = st.columns(4)
                        col1.metric("Tokens ƒê·∫ßu v√†o", usage_info['prompt_tokens'])
                        col2.metric("Tokens ƒê·∫ßu ra", usage_info['response_tokens'])
                        col3.metric("T·ªïng Tokens", usage_info['total_tokens'])
                        col4.metric("Chi ph√≠ (USD)", f"${usage_info['cost_usd']:.6f}")
                        st.caption(f"M√¥ h√¨nh s·ª≠ d·ª•ng: `{usage_info['model']}`")

        # L∆∞u l·∫°i to√†n b·ªô n·ªôi dung ƒë√£ hi·ªÉn th·ªã v√†o l·ªãch s·ª≠ chat
        if usage_info:
            usage_html = f"""
            <details>
                <summary>Xem chi ti·∫øt s·ª≠ d·ª•ng API</summary>
                <div style="padding: 10px; background-color: #f0f2f6; border-radius: 5px;">
                    <p><b>M√¥ h√¨nh:</b> {usage_info['model']}</p>
                    <p><b>Tokens ƒê·∫ßu v√†o:</b> {usage_info['prompt_tokens']}</p>
                    <p><b>Tokens ƒê·∫ßu ra:</b> {usage_info['response_tokens']}</p>
                    <p><b>T·ªïng Tokens:</b> {usage_info['total_tokens']}</p>
                    <p><b>Chi ph√≠ (USD):</b> ${usage_info['cost_usd']:.6f}</p>
                </div>
            </details>
            """
            full_response += usage_html

        st.session_state.messages.append({"role": "assistant", "content": full_response})